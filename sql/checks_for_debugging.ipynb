{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "08b74eae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== FOREIGN KEY DEBUG ===\n",
      "Categories in database: 74\n",
      "Categories in CSV: 71\n",
      "Unique categories in products CSV: 73\n",
      "Categories in translation CSV: 71\n",
      "\n",
      "Missing categories (in products but not in translation): 2\n",
      "Missing categories: ['pc_gamer', 'portateis_cozinha_e_preparadores_de_alimentos']\n",
      "\n",
      "NULL categories in products: 610\n",
      "\n",
      "First 10 categories in database: ['agro_industria_e_comercio', 'alimentos', 'alimentos_bebidas', 'artes', 'artes_e_artesanato', 'artigos_de_festas', 'artigos_de_natal', 'audio', 'automotivo', 'bebes']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "\n",
    "# Check what's actually in the database vs CSV\n",
    "def debug_foreign_key_issue():\n",
    "    # Read the CSV files\n",
    "    products_df = pd.read_csv('../data/olist_products_dataset.csv')\n",
    "    categories_df = pd.read_csv('../data/product_category_name_translation.csv')\n",
    "    \n",
    "    print(\"=== FOREIGN KEY DEBUG ===\")\n",
    "    \n",
    "    # Check categories loaded in database\n",
    "    conn = sqlite3.connect('../olist_dashboard.db')\n",
    "    \n",
    "    db_categories = pd.read_sql(\"SELECT product_category_name FROM dim_product_categories\", conn)\n",
    "    print(f\"Categories in database: {len(db_categories)}\")\n",
    "    print(f\"Categories in CSV: {len(categories_df)}\")\n",
    "    \n",
    "    # Check unique categories in products CSV\n",
    "    product_categories = products_df['product_category_name'].dropna().unique()\n",
    "    csv_categories = categories_df['product_category_name'].unique()\n",
    "    \n",
    "    print(f\"Unique categories in products CSV: {len(product_categories)}\")\n",
    "    print(f\"Categories in translation CSV: {len(csv_categories)}\")\n",
    "    \n",
    "    # Find missing categories\n",
    "    missing_categories = set(product_categories) - set(csv_categories)\n",
    "    print(f\"\\nMissing categories (in products but not in translation): {len(missing_categories)}\")\n",
    "    if missing_categories:\n",
    "        print(\"Missing categories:\", list(missing_categories)[:10])  # Show first 10\n",
    "    \n",
    "    # Check NULL values\n",
    "    null_categories = products_df['product_category_name'].isnull().sum()\n",
    "    print(f\"\\nNULL categories in products: {null_categories}\")\n",
    "    \n",
    "    # Check what's actually in the database\n",
    "    db_category_list = db_categories['product_category_name'].tolist()\n",
    "    print(f\"\\nFirst 10 categories in database: {db_category_list[:10]}\")\n",
    "    \n",
    "    conn.close()\n",
    "    \n",
    "    return missing_categories\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    missing = debug_foreign_key_issue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3687b2fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== NULL CATEGORY PRODUCTS ANALYSIS ===\n",
      "Products with NULL categories: 610\n",
      "Products with valid categories: 32341\n",
      "\n",
      "=== NULL PRODUCTS DATA QUALITY ===\n",
      "Data completeness for NULL category products:\n",
      "  product_id: 100.0% complete (610/610)\n",
      "  product_name_lenght: 0.0% complete (0/610)\n",
      "  product_description_lenght: 0.0% complete (0/610)\n",
      "  product_photos_qty: 0.0% complete (0/610)\n",
      "  product_weight_g: 99.8% complete (609/610)\n",
      "  product_length_cm: 99.8% complete (609/610)\n",
      "  product_height_cm: 99.8% complete (609/610)\n",
      "  product_width_cm: 99.8% complete (609/610)\n",
      "\n",
      "=== SAMPLE NULL CATEGORY PRODUCTS ===\n",
      "                           product_id  product_name_lenght  product_weight_g  \\\n",
      "105  a41e356c76fab66334f36de622ecbd3a                  NaN             650.0   \n",
      "128  d8dee61c2034d6d075997acef1870e9b                  NaN             300.0   \n",
      "145  56139431d72cd51f19eb9f7dae4d1617                  NaN             200.0   \n",
      "154  46b48281eb6d663ced748f324108c733                  NaN           18500.0   \n",
      "197  5fb61f482620cb672f5e586bb132eae9                  NaN             300.0   \n",
      "244  e10758160da97891c2fdcbc35f0f031d                  NaN            2200.0   \n",
      "294  39e3b9b12cd0bf8ee681bbc1c130feb5                  NaN             300.0   \n",
      "299  794de06c32a626a5692ff50e4985d36f                  NaN             300.0   \n",
      "347  7af3e2da474486a3519b0cba9dea8ad9                  NaN             200.0   \n",
      "428  629beb8e7317703dcc5f35b5463fd20e                  NaN            1400.0   \n",
      "\n",
      "     product_length_cm  product_height_cm  product_width_cm  \n",
      "105               17.0               14.0              12.0  \n",
      "128               16.0                7.0              20.0  \n",
      "145               20.0               20.0              20.0  \n",
      "154               41.0               30.0              41.0  \n",
      "197               35.0                7.0              12.0  \n",
      "244               16.0                2.0              11.0  \n",
      "294               16.0                7.0              11.0  \n",
      "299               18.0                8.0              14.0  \n",
      "347               22.0               14.0              14.0  \n",
      "428               25.0               25.0              25.0  \n",
      "\n",
      "=== NULL PRODUCTS IN ORDERS ===\n",
      "NULL category products that were actually sold: 1603\n",
      "Unique NULL products sold: 610\n",
      "Total revenue from NULL category products: $179,535.28\n",
      "\n",
      "=== MISSING CATEGORIES ANALYSIS ===\n",
      "\n",
      "Category: pc_gamer\n",
      "Number of products: 3\n",
      "Products sold: 3\n",
      "Revenue: $1,545.95\n",
      "Average price: $171.77\n",
      "\n",
      "Category: portateis_cozinha_e_preparadores_de_alimentos\n",
      "Number of products: 10\n",
      "Products sold: 10\n",
      "Revenue: $3,968.53\n",
      "Average price: $264.57\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def analyze_null_products():\n",
    "    \"\"\"Analyze products with NULL categories to see if they're worth keeping\"\"\"\n",
    "    \n",
    "    df = pd.read_csv('../data/olist_products_dataset.csv')\n",
    "    \n",
    "    print(\"=== NULL CATEGORY PRODUCTS ANALYSIS ===\")\n",
    "    \n",
    "    # Separate NULL and non-NULL products\n",
    "    null_products = df[df['product_category_name'].isnull()]\n",
    "    valid_products = df[df['product_category_name'].notnull()]\n",
    "    \n",
    "    print(f\"Products with NULL categories: {len(null_products)}\")\n",
    "    print(f\"Products with valid categories: {len(valid_products)}\")\n",
    "    \n",
    "    # Check if NULL products have other useful data\n",
    "    print(f\"\\n=== NULL PRODUCTS DATA QUALITY ===\")\n",
    "    \n",
    "    # Check non-category columns for completeness\n",
    "    null_cols_analysis = {}\n",
    "    for col in null_products.columns:\n",
    "        if col != 'product_category_name':\n",
    "            non_null_count = null_products[col].notnull().sum()\n",
    "            null_cols_analysis[col] = {\n",
    "                'non_null': non_null_count,\n",
    "                'null': len(null_products) - non_null_count,\n",
    "                'completeness_pct': (non_null_count / len(null_products)) * 100\n",
    "            }\n",
    "    \n",
    "    print(\"Data completeness for NULL category products:\")\n",
    "    for col, stats in null_cols_analysis.items():\n",
    "        print(f\"  {col}: {stats['completeness_pct']:.1f}% complete ({stats['non_null']}/{len(null_products)})\")\n",
    "    \n",
    "    # Sample NULL products to see what they look like\n",
    "    print(f\"\\n=== SAMPLE NULL CATEGORY PRODUCTS ===\")\n",
    "    print(null_products[['product_id', 'product_name_lenght', 'product_weight_g', \n",
    "                        'product_length_cm', 'product_height_cm', 'product_width_cm']].head(10))\n",
    "    \n",
    "    # Check if NULL products are referenced in order_items (i.e., actually sold)\n",
    "    order_items = pd.read_csv('../data/olist_order_items_dataset.csv')\n",
    "    null_product_ids = set(null_products['product_id'])\n",
    "    sold_null_products = order_items[order_items['product_id'].isin(null_product_ids)]\n",
    "    \n",
    "    print(f\"\\n=== NULL PRODUCTS IN ORDERS ===\")\n",
    "    print(f\"NULL category products that were actually sold: {len(sold_null_products)}\")\n",
    "    print(f\"Unique NULL products sold: {sold_null_products['product_id'].nunique()}\")\n",
    "    \n",
    "    if len(sold_null_products) > 0:\n",
    "        total_revenue = sold_null_products['price'].sum()\n",
    "        print(f\"Total revenue from NULL category products: ${total_revenue:,.2f}\")\n",
    "    \n",
    "    return null_products, sold_null_products\n",
    "\n",
    "def analyze_missing_categories():\n",
    "    \"\"\"Analyze the 2 missing categories\"\"\"\n",
    "    \n",
    "    df = pd.read_csv('../data/olist_products_dataset.csv')\n",
    "    \n",
    "    missing_cats = ['pc_gamer', 'portateis_cozinha_e_preparadores_de_alimentos']\n",
    "    \n",
    "    print(f\"\\n=== MISSING CATEGORIES ANALYSIS ===\")\n",
    "    \n",
    "    for cat in missing_cats:\n",
    "        cat_products = df[df['product_category_name'] == cat]\n",
    "        print(f\"\\nCategory: {cat}\")\n",
    "        print(f\"Number of products: {len(cat_products)}\")\n",
    "        \n",
    "        if len(cat_products) > 0:\n",
    "            # Check if these products are sold\n",
    "            order_items = pd.read_csv('../data/olist_order_items_dataset.csv')\n",
    "            cat_product_ids = set(cat_products['product_id'])\n",
    "            sold_cat_products = order_items[order_items['product_id'].isin(cat_product_ids)]\n",
    "            \n",
    "            print(f\"Products sold: {sold_cat_products['product_id'].nunique()}\")\n",
    "            if len(sold_cat_products) > 0:\n",
    "                revenue = sold_cat_products['price'].sum()\n",
    "                print(f\"Revenue: ${revenue:,.2f}\")\n",
    "                avg_price = sold_cat_products['price'].mean()\n",
    "                print(f\"Average price: ${avg_price:.2f}\")\n",
    "\n",
    "null_products, sold_nulls = analyze_null_products()\n",
    "analyze_missing_categories()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2f0f59",
   "metadata": {},
   "source": [
    "NULL Category Products:\n",
    "\n",
    "610 products with NULL categories that were ALL sold (100% sell-through)\n",
    "$179,535 in revenue - significant business impact\n",
    "Physical dimensions are 99.8% complete - good quality data\n",
    "Only missing description/name length and photo count (not critical for analysis)\n",
    "\n",
    "\n",
    "ADD NULL CATEGORY IN SCHEMA AND ADD MISSING PRODUCT CATEGORIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "90dbe4d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ORDER STATUS ANALYSIS ===\n",
      "Unique statuses in data: 8\n",
      "Statuses found: ['approved', 'canceled', 'created', 'delivered', 'invoiced', 'processing', 'shipped', 'unavailable']\n",
      "\n",
      "Allowed in schema: ['delivered', 'shipped', 'processing', 'unavailable', 'canceled', 'approved', 'invoiced']\n",
      "\n",
      "Invalid statuses: ['created']\n",
      "\n",
      "Status counts:\n",
      "  ✓ approved: 2 orders\n",
      "  ✓ canceled: 625 orders\n",
      "  ✗ created: 5 orders\n",
      "  ✓ delivered: 96,478 orders\n",
      "  ✓ invoiced: 314 orders\n",
      "  ✓ processing: 301 orders\n",
      "  ✓ shipped: 1,107 orders\n",
      "  ✓ unavailable: 609 orders\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def check_order_statuses():\n",
    "    \"\"\"Check what order statuses exist in the data\"\"\"\n",
    "    \n",
    "    df = pd.read_csv('../data/olist_orders_dataset2.csv')\n",
    "    \n",
    "    print(\"=== ORDER STATUS ANALYSIS ===\")\n",
    "    \n",
    "    # Get all unique statuses\n",
    "    all_statuses = df['order_status'].unique()\n",
    "    print(f\"Unique statuses in data: {len(all_statuses)}\")\n",
    "    print(\"Statuses found:\", sorted(all_statuses))\n",
    "    \n",
    "    # Schema allowed statuses\n",
    "    allowed_statuses = ['delivered', 'shipped', 'processing', 'unavailable', 'canceled', 'approved', 'invoiced']\n",
    "    print(f\"\\nAllowed in schema: {allowed_statuses}\")\n",
    "    \n",
    "    # Find invalid statuses\n",
    "    invalid_statuses = set(all_statuses) - set(allowed_statuses)\n",
    "    print(f\"\\nInvalid statuses: {sorted(invalid_statuses)}\")\n",
    "    \n",
    "    # Count orders for each status\n",
    "    status_counts = df['order_status'].value_counts()\n",
    "    print(f\"\\nStatus counts:\")\n",
    "    for status in sorted(all_statuses):\n",
    "        count = status_counts[status]\n",
    "        valid = \"✓\" if status in allowed_statuses else \"✗\"\n",
    "        print(f\"  {valid} {status}: {count:,} orders\")\n",
    "    \n",
    "    return invalid_statuses\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    invalid = check_order_statuses()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5412d192",
   "metadata": {},
   "source": [
    "Now you have duplicate review_id values in your reviews data. This means your CSV file has the same review ID appearing multiple times, which violates the PRIMARY KEY constraint. -- CHECK DUPLICATES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a4051e03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== REVIEW DUPLICATES ANALYSIS ===\n",
      "Total reviews: 99,224\n",
      "Unique review IDs: 98,410\n",
      "Duplicate reviews: 814\n",
      "\n",
      "Number of review IDs with duplicates: 789\n",
      "\n",
      "Sample duplicate review IDs:\n",
      "\n",
      "Review ID: 28642ce6250b94cc72bc85960aec6c62\n",
      "Appears 2 times\n",
      "                       review_id                         order_id  review_score review_creation_date review_comment_message\n",
      "28642ce6250b94cc72bc85960aec6c62 e239d280236cdd3c40cb2c033f681d1c             5  2018-03-25 00:00:00                    NaN\n",
      "28642ce6250b94cc72bc85960aec6c62 bc42a955f289870d5789e6e437206300             5  2018-03-25 00:00:00                    NaN\n",
      "\n",
      "Review ID: a0a641414ff718ca079b3967ef5c2495\n",
      "Appears 2 times\n",
      "                       review_id                         order_id  review_score review_creation_date review_comment_message\n",
      "a0a641414ff718ca079b3967ef5c2495 169d7e0fd71d624d306f132acd791cbe             5  2018-03-04 00:00:00                    NaN\n",
      "a0a641414ff718ca079b3967ef5c2495 4e93b736e8d687bca088c6ee496437e8             5  2018-03-04 00:00:00                    NaN\n",
      "\n",
      "Review ID: f4d74b17cd63ee35efa82cd2567de911\n",
      "Appears 2 times\n",
      "                       review_id                         order_id  review_score review_creation_date                                                                              review_comment_message\n",
      "f4d74b17cd63ee35efa82cd2567de911 f269e83a82f64baa3de97c2ebf3358f6             3  2018-01-12 00:00:00 A embalagem deixou a desejar, por pouco o produto não foi danificado, a caixa estava toda amassada.\n",
      "f4d74b17cd63ee35efa82cd2567de911 a60c9bf6dcdd39e9f1c10ae5ad77c24d             3  2018-01-12 00:00:00 A embalagem deixou a desejar, por pouco o produto não foi danificado, a caixa estava toda amassada.\n",
      "\n",
      "Review ID: ecbaf1fce7d2c09bfab46f89065afeaf\n",
      "Appears 2 times\n",
      "                       review_id                         order_id  review_score review_creation_date review_comment_message\n",
      "ecbaf1fce7d2c09bfab46f89065afeaf 2451b9756f310d4cff5c7987b393870d             5  2017-07-27 00:00:00                    NaN\n",
      "ecbaf1fce7d2c09bfab46f89065afeaf a67f04f0a3022835f86664a0070fe36e             5  2017-07-27 00:00:00                    NaN\n",
      "\n",
      "Review ID: 6b1de94de0f4bd84dfc4136818242faa\n",
      "Appears 2 times\n",
      "                       review_id                         order_id  review_score review_creation_date review_comment_message\n",
      "6b1de94de0f4bd84dfc4136818242faa 92acf87839903a94aeca0e5040d99acb             5  2018-02-16 00:00:00                    NaN\n",
      "6b1de94de0f4bd84dfc4136818242faa 9de125e032c8c74ccbcf9a5b7a3b4faa             5  2018-02-16 00:00:00                    NaN\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def check_review_duplicates():\n",
    "    \"\"\"Check for duplicate review IDs in the data\"\"\"\n",
    "    \n",
    "    df = pd.read_csv('../data/olist_order_reviews_dataset.csv')\n",
    "    \n",
    "    print(\"=== REVIEW DUPLICATES ANALYSIS ===\")\n",
    "    \n",
    "    total_reviews = len(df)\n",
    "    unique_review_ids = df['review_id'].nunique()\n",
    "    \n",
    "    print(f\"Total reviews: {total_reviews:,}\")\n",
    "    print(f\"Unique review IDs: {unique_review_ids:,}\")\n",
    "    print(f\"Duplicate reviews: {total_reviews - unique_review_ids:,}\")\n",
    "    \n",
    "    if total_reviews != unique_review_ids:\n",
    "        # Find duplicates\n",
    "        duplicates = df[df['review_id'].duplicated(keep=False)]\n",
    "        duplicate_ids = duplicates['review_id'].unique()\n",
    "        \n",
    "        print(f\"\\nNumber of review IDs with duplicates: {len(duplicate_ids)}\")\n",
    "        print(\"\\nSample duplicate review IDs:\")\n",
    "        for review_id in duplicate_ids[:5]:  # Show first 5\n",
    "            dup_records = df[df['review_id'] == review_id]\n",
    "            print(f\"\\nReview ID: {review_id}\")\n",
    "            print(f\"Appears {len(dup_records)} times\")\n",
    "            print(dup_records[['review_id', 'order_id', 'review_score', 'review_creation_date', 'review_comment_message']].to_string(index=False))\n",
    "    \n",
    "    return total_reviews - unique_review_ids\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    duplicates = check_review_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c9fd39",
   "metadata": {},
   "source": [
    "The duplicates show that 814 reviews have the same review_id but different order_ids. This suggests the same review was applied to multiple orders - possibly copy-paste reviews or system errors.\n",
    "Looking at the pattern:\n",
    "\n",
    "Same review_id, same review_score, same creation_date\n",
    "Different order_ids\n",
    "Often same review_comment_message\n",
    "\n",
    "--> FIX LOAD REVIEWS --> sqlite3 olist_dashboard.db < sql/01_create_schema.sql "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
