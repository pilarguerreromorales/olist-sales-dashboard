{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "415db38b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OLIST DATASET QUICK OVERVIEW\n",
      "============================================================\n",
      "Found 9 CSV files\n",
      "\n",
      "SELLERS\n",
      "--------------------------------------------------\n",
      "Shape: 3,095 rows √ó 4 columns\n",
      "Size: 0.6 MB\n",
      "\n",
      " Columns (4):\n",
      "   1. seller_id\n",
      "   2. seller_zip_code_prefix\n",
      "   3. seller_city\n",
      "   4. seller_state\n",
      "\n",
      "First 3 rows:\n",
      "             seller_id  seller_zip_code_prefix     seller_city seller_state\n",
      "0  3442f8959a84dea7...                13023           campinas           SP\n",
      "1  d1b65fc7debc3361...                13844         mogi guacu           SP\n",
      "2  ce3ad9de960102d0...                20031     rio de janeiro           RJ\n",
      "\n",
      "============================================================\n",
      "\n",
      "PRODUCT_CATEGORY_NAME_TRANSLATION\n",
      "--------------------------------------------------\n",
      "Shape: 71 rows √ó 2 columns\n",
      "Size: 0.0 MB\n",
      "\n",
      " Columns (2):\n",
      "   1. product_category_name\n",
      "   2. product_category_name_english\n",
      "\n",
      "First 3 rows:\n",
      "  product_category_name product_category_name_english\n",
      "0         beleza_saude         health_beauty         \n",
      "1  informatica_aces...   computers_access...         \n",
      "2           automotivo                  auto         \n",
      "\n",
      "============================================================\n",
      "\n",
      "ORDERS\n",
      "--------------------------------------------------\n",
      "Shape: 99,441 rows √ó 8 columns\n",
      "Size: 52.9 MB\n",
      "\n",
      " Columns (8):\n",
      "   1. order_id\n",
      "   2. customer_id\n",
      "   3. order_status\n",
      "   4. order_purchase_timestamp\n",
      "   5. order_approved_at\n",
      "   6. order_delivered_carrier_date\n",
      "   7. order_delivered_customer_date\n",
      "   8. order_estimated_delivery_date\n",
      "\n",
      "First 3 rows:\n",
      "              order_id          customer_id order_status  ... order_delivered_carrier_date order_delivered_customer_date order_estimated_delivery_date\n",
      "0  e481f51cbdc54678...  9ef432eb62512973...    delivered  ...  2017-10-04 19:55:00          2017-10-10 21:25:13           2017-10-18 00:00:00         \n",
      "1  53cdb2fc8bc7dce0...  b0830fb4747a6c6d...    delivered  ...  2018-07-26 14:31:00          2018-08-07 15:27:45           2018-08-13 00:00:00         \n",
      "2  47770eb9100c2d0c...  41ce2a54c0b03bf3...    delivered  ...  2018-08-08 13:50:00          2018-08-17 18:06:29           2018-09-04 00:00:00         \n",
      "\n",
      "============================================================\n",
      "\n",
      "ORDER_ITEMS\n",
      "--------------------------------------------------\n",
      "Shape: 112,650 rows √ó 7 columns\n",
      "Size: 36.0 MB\n",
      "\n",
      " Columns (7):\n",
      "   1. order_id\n",
      "   2. order_item_id\n",
      "   3. product_id\n",
      "   4. seller_id\n",
      "   5. shipping_limit_date\n",
      "   6. price\n",
      "   7. freight_value\n",
      "\n",
      "First 3 rows:\n",
      "              order_id  order_item_id           product_id  ...  shipping_limit_date  price  freight_value\n",
      "0  00010242fe8c5a6d...              1  4244733e06e7ecb4...  ...  2017-09-19 09:45:35   58.9          13.29\n",
      "1  00018f77f2f0320c...              1  e5f2d52b802189ee...  ...  2017-05-03 11:05:13  239.9          19.93\n",
      "2  000229ec398224ef...              1  c777355d18b72b67...  ...  2018-01-18 14:48:30  199.0          17.87\n",
      "\n",
      "============================================================\n",
      "\n",
      "CUSTOMERS\n",
      "--------------------------------------------------\n",
      "Shape: 99,441 rows √ó 5 columns\n",
      "Size: 26.6 MB\n",
      "\n",
      " Columns (5):\n",
      "   1. customer_id\n",
      "   2. customer_unique_id\n",
      "   3. customer_zip_code_prefix\n",
      "   4. customer_city\n",
      "   5. customer_state\n",
      "\n",
      "First 3 rows:\n",
      "           customer_id   customer_unique_id  customer_zip_code_prefix        customer_city customer_state\n",
      "0  06b8999e2fba1a1f...  861eff4711a542e4...                14409                    franca             SP\n",
      "1  18955e83d337fd6b...  290c77bc529b7ac9...                 9790       sao bernardo do ...             SP\n",
      "2  4e7b3e00288586eb...  060e732b5b29e818...                 1151                 sao paulo             SP\n",
      "\n",
      "============================================================\n",
      "\n",
      "GEOLOCATION\n",
      "--------------------------------------------------\n",
      "Shape: 1,000,163 rows √ó 5 columns\n",
      "Size: 129.4 MB\n",
      "\n",
      " Columns (5):\n",
      "   1. geolocation_zip_code_prefix\n",
      "   2. geolocation_lat\n",
      "   3. geolocation_lng\n",
      "   4. geolocation_city\n",
      "   5. geolocation_state\n",
      "\n",
      "First 3 rows:\n",
      "   geolocation_zip_code_prefix  geolocation_lat  geolocation_lng geolocation_city geolocation_state\n",
      "0                 1037               -23.545621       -46.639292        sao paulo                SP\n",
      "1                 1046               -23.546081       -46.644820        sao paulo                SP\n",
      "2                 1046               -23.546129       -46.642951        sao paulo                SP\n",
      "\n",
      "============================================================\n",
      "\n",
      "ORDER_PAYMENTS\n",
      "--------------------------------------------------\n",
      "Shape: 103,886 rows √ó 5 columns\n",
      "Size: 16.2 MB\n",
      "\n",
      " Columns (5):\n",
      "   1. order_id\n",
      "   2. payment_sequential\n",
      "   3. payment_type\n",
      "   4. payment_installments\n",
      "   5. payment_value\n",
      "\n",
      "First 3 rows:\n",
      "              order_id  payment_sequential payment_type  payment_installments  payment_value\n",
      "0  b81ef226f3fe1789...                   1  credit_card                    8           99.33\n",
      "1  a9810da82917af2d...                   1  credit_card                    1           24.39\n",
      "2  25e8ea4e93396b6f...                   1  credit_card                    1           65.71\n",
      "\n",
      "============================================================\n",
      "\n",
      "ORDER_REVIEWS\n",
      "--------------------------------------------------\n",
      "Shape: 99,224 rows √ó 7 columns\n",
      "Size: 39.1 MB\n",
      "\n",
      " Columns (7):\n",
      "   1. review_id\n",
      "   2. order_id\n",
      "   3. review_score\n",
      "   4. review_comment_title\n",
      "   5. review_comment_message\n",
      "   6. review_creation_date\n",
      "   7. review_answer_timestamp\n",
      "\n",
      "First 3 rows:\n",
      "             review_id             order_id  review_score  ... review_comment_message review_creation_date review_answer_timestamp\n",
      "0  7bc2406110b92639...  73fc7af87114b397...             4  ...                  NaN    2018-01-18 00:00:00  2018-01-18 21:46:59   \n",
      "1  80e641a11e56f04c...  a548910a1c614779...             5  ...                  NaN    2018-03-10 00:00:00  2018-03-11 03:05:13   \n",
      "2  228ce5500dc1d8e0...  f9e4b658b201a9f2...             5  ...                  NaN    2018-02-17 00:00:00  2018-02-18 14:36:24   \n",
      "\n",
      "============================================================\n",
      "\n",
      "PRODUCTS\n",
      "--------------------------------------------------\n",
      "Shape: 32,951 rows √ó 9 columns\n",
      "Size: 6.3 MB\n",
      "\n",
      " Columns (9):\n",
      "   1. product_id\n",
      "   2. product_category_name\n",
      "   3. product_name_lenght\n",
      "   4. product_description_lenght\n",
      "   5. product_photos_qty\n",
      "   6. product_weight_g\n",
      "   7. product_length_cm\n",
      "   8. product_height_cm\n",
      "   9. product_width_cm\n",
      "\n",
      "First 3 rows:\n",
      "            product_id product_category_name  product_name_lenght  ...  product_length_cm  product_height_cm  product_width_cm\n",
      "0  1e9e8ef04dbcff45...           perfumaria                  40.0  ...               16.0               10.0              14.0\n",
      "1  3aa071139cb16b67...                artes                  44.0  ...               30.0               18.0              20.0\n",
      "2  96bd76ec8810374e...        esporte_lazer                  46.0  ...               18.0                9.0              15.0\n",
      "\n",
      "============================================================\n",
      "\n",
      "SUMMARY OF ALL DATASETS:\n",
      "============================================================\n",
      "sellers                      3,095 rows √ó  4 cols\n",
      "product_category_name_translation       71 rows √ó  2 cols\n",
      "orders                      99,441 rows √ó  8 cols\n",
      "order_items                112,650 rows √ó  7 cols\n",
      "customers                   99,441 rows √ó  5 cols\n",
      "geolocation               1,000,163 rows √ó  5 cols\n",
      "order_payments             103,886 rows √ó  5 cols\n",
      "order_reviews               99,224 rows √ó  7 cols\n",
      "products                    32,951 rows √ó  9 cols\n",
      "------------------------------------------------------------\n",
      "TOTAL                     1,550,922 rows √ó 52 cols\n"
     ]
    }
   ],
   "source": [
    "# Quick Dataset Overview - Run this in Jupyter or Python\n",
    "# Create as: notebooks/quick_overview.ipynb\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "print(\"OLIST DATASET QUICK OVERVIEW\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Data folder path (adjust if needed)\n",
    "DATA_PATH = \"../data/\"\n",
    "\n",
    "# Get all CSV files\n",
    "try:\n",
    "    csv_files = [f for f in os.listdir(DATA_PATH) if f.endswith('.csv')]\n",
    "    print(f\"Found {len(csv_files)} CSV files\\n\")\n",
    "except:\n",
    "    print(\"Data folder not found. Make sure you have a 'data' folder with CSV files\")\n",
    "    csv_files = []\n",
    "\n",
    "# Quick overview function\n",
    "def quick_overview(filename):\n",
    "    \"\"\"Show columns and first few rows of a dataset\"\"\"\n",
    "    try:\n",
    "        # Load the dataset\n",
    "        df = pd.read_csv(DATA_PATH + filename)\n",
    "        \n",
    "        # Clean filename for display\n",
    "        display_name = filename.replace('olist_', '').replace('_dataset.csv', '').replace('.csv', '').upper()\n",
    "        \n",
    "        print(f\"{display_name}\")\n",
    "        print(\"-\" * 50)\n",
    "        print(f\"Shape: {df.shape[0]:,} rows √ó {df.shape[1]} columns\")\n",
    "        print(f\"Size: {df.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")\n",
    "        \n",
    "        print(f\"\\n Columns ({len(df.columns)}):\")\n",
    "        for i, col in enumerate(df.columns, 1):\n",
    "            print(f\"  {i:2d}. {col}\")\n",
    "        \n",
    "        print(f\"\\nFirst 3 rows:\")\n",
    "        print(df.head(3).to_string(max_cols=6, max_colwidth=20))\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "        \n",
    "        return df.shape, list(df.columns)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {filename}: {e}\")\n",
    "        return None, None\n",
    "\n",
    "# Overview all datasets\n",
    "dataset_info = {}\n",
    "\n",
    "for filename in csv_files:\n",
    "    shape, columns = quick_overview(filename)\n",
    "    if shape:\n",
    "        dataset_info[filename] = {\n",
    "            'shape': shape,\n",
    "            'columns': columns\n",
    "        }\n",
    "\n",
    "# Summary\n",
    "if dataset_info:\n",
    "    print(\"SUMMARY OF ALL DATASETS:\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    total_rows = sum([info['shape'][0] for info in dataset_info.values()])\n",
    "    total_columns = sum([info['shape'][1] for info in dataset_info.values()])\n",
    "    \n",
    "    for filename, info in dataset_info.items():\n",
    "        clean_name = filename.replace('olist_', '').replace('_dataset.csv', '').replace('.csv', '')\n",
    "        print(f\"{clean_name:<25} {info['shape'][0]:>8,} rows √ó {info['shape'][1]:>2} cols\")\n",
    "    \n",
    "    print(\"-\" * 60)\n",
    "    print(f\"{'TOTAL':<25} {total_rows:>8,} rows √ó {total_columns:>2} cols\")\n",
    "    \n",
    "\n",
    "else:\n",
    "    print(\"No datasets loaded. Check your data folder!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2aff4ee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (99441, 8)\n",
      "Total orders: 99441\n",
      "\n",
      "Time Range:\n",
      "Start date: 2016-09-04 21:15:19\n",
      "End date: 2018-10-17 17:30:18\n",
      "Total days: 772\n",
      "Total years: 2.1\n",
      "\n",
      "Timestamp format:\n",
      "Sample timestamps:\n",
      "[Timestamp('2017-10-02 10:56:33'), Timestamp('2018-07-24 20:41:37'), Timestamp('2018-08-08 08:38:49')]\n",
      "\n",
      "Orders by year:\n",
      "2016: 329 orders\n",
      "2017: 45,101 orders\n",
      "2018: 54,011 orders\n",
      "\n",
      "Orders by month:\n",
      "Month 1: 8,069 orders\n",
      "Month 2: 8,508 orders\n",
      "Month 3: 9,893 orders\n",
      "Month 4: 9,343 orders\n",
      "Month 5: 10,573 orders\n",
      "Month 6: 9,412 orders\n",
      "Month 7: 10,318 orders\n",
      "Month 8: 10,843 orders\n",
      "Month 9: 4,305 orders\n",
      "Month 10: 4,959 orders\n",
      "Month 11: 7,544 orders\n",
      "Month 12: 5,674 orders\n"
     ]
    }
   ],
   "source": [
    "# Simple Time Range Analysis for Olist Orders\n",
    "import pandas as pd\n",
    "\n",
    "# Load orders dataset\n",
    "df = pd.read_csv(\"../data/olist_orders_dataset.csv\")\n",
    "\n",
    "# Convert to datetime\n",
    "df['order_purchase_timestamp'] = pd.to_datetime(df['order_purchase_timestamp'])\n",
    "\n",
    "# Basic info\n",
    "print(\"Dataset shape:\", df.shape)\n",
    "print(\"Total orders:\", len(df))\n",
    "\n",
    "# Time range\n",
    "start_date = df['order_purchase_timestamp'].min()\n",
    "end_date = df['order_purchase_timestamp'].max()\n",
    "total_days = (end_date - start_date).days\n",
    "\n",
    "print(\"\\nTime Range:\")\n",
    "print(\"Start date:\", start_date)\n",
    "print(\"End date:\", end_date)\n",
    "print(\"Total days:\", total_days)\n",
    "print(\"Total years:\", round(total_days/365.25, 1))\n",
    "\n",
    "# Check timestamp format\n",
    "print(\"\\nTimestamp format:\")\n",
    "print(\"Sample timestamps:\")\n",
    "print(df['order_purchase_timestamp'].head(3).tolist())\n",
    "\n",
    "# Data distribution by year\n",
    "print(\"\\nOrders by year:\")\n",
    "df['year'] = df['order_purchase_timestamp'].dt.year\n",
    "yearly_counts = df['year'].value_counts().sort_index()\n",
    "for year, count in yearly_counts.items():\n",
    "    print(f\"{year}: {count:,} orders\")\n",
    "\n",
    "# Data distribution by month\n",
    "print(\"\\nOrders by month:\")\n",
    "df['month'] = df['order_purchase_timestamp'].dt.month\n",
    "monthly_counts = df['month'].value_counts().sort_index()\n",
    "for month, count in monthly_counts.items():\n",
    "    print(f\"Month {month}: {count:,} orders\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35eb8131",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking for CSV files in: /Users/pilar/Desktop/Projects/olist-sales-dashboard/DataFoundation/../data\n",
      "Folder exists: True\n",
      "All files in folder: ['olist_orders_dataset2.csv', 'olist_sellers_dataset.csv', 'product_category_name_translation.csv', 'state_data_sources.csv', 'olist_orders_dataset.csv', 'olist_order_items_dataset.csv', 'economic_indicators.csv', 'olist_customers_dataset.csv', 'olist_geolocation_dataset.csv', 'olist_order_payments_dataset.csv', 'olitst_holiday_dataset.csv', 'state_enhancement_documented.csv', 'olist_order_reviews_dataset.csv', 'olist_products_dataset.csv', 'olist_holiday_dataset.csv']\n",
      "Found 15 CSV files: ['olist_orders_dataset2.csv', 'olist_sellers_dataset.csv', 'product_category_name_translation.csv', 'state_data_sources.csv', 'olist_orders_dataset.csv', 'olist_order_items_dataset.csv', 'economic_indicators.csv', 'olist_customers_dataset.csv', 'olist_geolocation_dataset.csv', 'olist_order_payments_dataset.csv', 'olitst_holiday_dataset.csv', 'state_enhancement_documented.csv', 'olist_order_reviews_dataset.csv', 'olist_products_dataset.csv', 'olist_holiday_dataset.csv']\n",
      "================================================================================\n",
      "CSV FILES ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "üìÅ FILE: economic_indicators.csv\n",
      "------------------------------------------------------------\n",
      "üìä SHAPE: 774 rows x 12 columns\n",
      "üíæ SIZE: 0.08 MB\n",
      "\n",
      "üìã COLUMNS:\n",
      "   1. purchase_date                  | object       |   0.0% null | Ex: 2016-09-04, 2016-09-05, 2016-09-06\n",
      "   2. usd_brl_rate                   | float64      |   0.0% null | Ex: 3.2721, 3.2721, 3.2452\n",
      "   3. selic_rate                     | float64      |   0.0% null | Ex: 0.052531, 0.052531, 0.052531\n",
      "   4. selic_target                   | float64      |   0.0% null | Ex: 14.25, 14.25, 14.25\n",
      "   5. ipca_inflation                 | float64      |   0.0% null | Ex: 0.26, 0.26, 0.26\n",
      "   6. usd_brl_change                 | float64      |   0.1% null | Ex: 0.0, -0.0082210201399712, -0.0079810181190681\n",
      "   7. usd_brl_volatility             | float64      |   0.8% null | Ex: 0.030964588069046, 0.0318255138738297, 0.0337719311460468\n",
      "   8. usd_brl_30day_avg              | float64      |   3.7% null | Ex: 3.2558700000000003, 3.2541433333333334, 3.2529366666666664\n",
      "   9. usd_brl_high                   | int64        |   0.0% null | Ex: 0, 0, 0\n",
      "  10. usd_brl_low                    | int64        |   0.0% null | Ex: 0, 0, 0\n",
      "  11. selic_changed                  | int64        |   0.0% null | Ex: 1, 0, 0\n",
      "  12. high_interest_period           | int64        |   0.0% null | Ex: 1, 1, 1\n",
      "\n",
      "üìà SAMPLE DATA (first 3 rows):\n",
      "  purchase_date  usd_brl_rate  selic_rate  ...  usd_brl_low  selic_changed  high_interest_period\n",
      "0    2016-09-04        3.2721    0.052531  ...            0              1                     1\n",
      "1    2016-09-05        3.2721    0.052531  ...            0              0                     1\n",
      "2    2016-09-06        3.2452    0.052531  ...            0              0                     1\n",
      "\n",
      "üìÖ DATE COLUMNS ANALYSIS:\n",
      "  purchase_date: ['2016-09-04', '2016-09-05', '2016-09-06']\n",
      "\n",
      "================================================================================\n",
      "\n",
      "üìÅ FILE: olist_customers_dataset.csv\n",
      "------------------------------------------------------------\n",
      "üìä SHAPE: 1,000 rows x 5 columns\n",
      "üíæ SIZE: 8.62 MB\n",
      "\n",
      "üìã COLUMNS:\n",
      "   1. customer_id                    | object       |   0.0% null | Ex: 06b8999e2fba1a1fbc88172c00ba8bc7, 18955e83d337fd6b2def6b18a428ac77, 4e7b3e00288586ebd08712fdd0374a03\n",
      "   2. customer_unique_id             | object       |   0.0% null | Ex: 861eff4711a542e4b93843c6dd7febb0, 290c77bc529b7ac935b93aa66c333dc3, 060e732b5b29e8181a18229c7b0b2b5e\n",
      "   3. customer_zip_code_prefix       | int64        |   0.0% null | Ex: 14409, 9790, 1151\n",
      "   4. customer_city                  | object       |   0.0% null | Ex: franca, sao bernardo do campo, sao paulo\n",
      "   5. customer_state                 | object       |   0.0% null | Ex: SP, SP, SP\n",
      "\n",
      "üìà SAMPLE DATA (first 3 rows):\n",
      "                     customer_id             customer_unique_id  customer_zip_code_prefix          customer_city customer_state\n",
      "0  06b8999e2fba1a1fbc88172c00...  861eff4711a542e4b93843c6dd...                     14409                 franca             SP\n",
      "1  18955e83d337fd6b2def6b18a4...  290c77bc529b7ac935b93aa66c...                      9790  sao bernardo do campo             SP\n",
      "2  4e7b3e00288586ebd08712fdd0...  060e732b5b29e8181a18229c7b...                      1151              sao paulo             SP\n",
      "\n",
      "üîë ID COLUMNS:\n",
      "  customer_id: 1,000 unique values (100.0% unique)\n",
      "  customer_unique_id: 999 unique values (99.9% unique)\n",
      "\n",
      "================================================================================\n",
      "\n",
      "üìÅ FILE: olist_geolocation_dataset.csv\n",
      "------------------------------------------------------------\n",
      "üìä SHAPE: 1,000 rows x 5 columns\n",
      "üíæ SIZE: 58.44 MB\n",
      "\n",
      "üìã COLUMNS:\n",
      "   1. geolocation_zip_code_prefix    | int64        |   0.0% null | Ex: 1037, 1046, 1046\n",
      "   2. geolocation_lat                | float64      |   0.0% null | Ex: -23.54562128115268, -23.546081127035535, -23.54612896641469\n",
      "   3. geolocation_lng                | float64      |   0.0% null | Ex: -46.63929204800168, -46.64482029837157, -46.64295148361138\n",
      "   4. geolocation_city               | object       |   0.0% null | Ex: sao paulo, sao paulo, sao paulo\n",
      "   5. geolocation_state              | object       |   0.0% null | Ex: SP, SP, SP\n",
      "\n",
      "üìà SAMPLE DATA (first 3 rows):\n",
      "   geolocation_zip_code_prefix  geolocation_lat  geolocation_lng geolocation_city geolocation_state\n",
      "0                         1037       -23.545621       -46.639292        sao paulo                SP\n",
      "1                         1046       -23.546081       -46.644820        sao paulo                SP\n",
      "2                         1046       -23.546129       -46.642951        sao paulo                SP\n",
      "\n",
      "================================================================================\n",
      "\n",
      "üìÅ FILE: olist_holiday_dataset.csv\n",
      "------------------------------------------------------------\n",
      "üìä SHAPE: 774 rows x 16 columns\n",
      "üíæ SIZE: 0.03 MB\n",
      "\n",
      "üìã COLUMNS:\n",
      "   1. purchase_date                  | object       |   0.0% null | Ex: 2016-09-04, 2016-09-05, 2016-09-06\n",
      "   2. weekday                        | int64        |   0.0% null | Ex: 6, 0, 1\n",
      "   3. month                          | int64        |   0.0% null | Ex: 9, 9, 9\n",
      "   4. is_holiday                     | int64        |   0.0% null | Ex: 0, 0, 0\n",
      "   5. holiday_name                   | object       |  97.4% null | Ex: Independ√™ncia do Brasil, Nossa Senhora Aparecida, Finados\n",
      "   6. is_carnival                    | int64        |   0.0% null | Ex: 0, 0, 0\n",
      "   7. is_weekend                     | int64        |   0.0% null | Ex: 1, 0, 0\n",
      "   8. is_friday                      | int64        |   0.0% null | Ex: 0, 0, 0\n",
      "   9. season                         | object       |   0.0% null | Ex: Spring, Spring, Spring\n",
      "  10. christmas_season               | int64        |   0.0% null | Ex: 0, 0, 0\n",
      "  11. is_major_event                 | int64        |   0.0% null | Ex: 0, 0, 0\n",
      "  12. is_shopping_holiday            | int64        |   0.0% null | Ex: 0, 0, 0\n",
      "  13. day_of_month                   | int64        |   0.0% null | Ex: 4, 5, 6\n",
      "  14. is_mid_month                   | int64        |   0.0% null | Ex: 0, 0, 0\n",
      "  15. is_last_3_days                 | int64        |   0.0% null | Ex: 0, 0, 0\n",
      "  16. is_day_24_non_december         | int64        |   0.0% null | Ex: 0, 0, 0\n",
      "\n",
      "üìà SAMPLE DATA (first 3 rows):\n",
      "  purchase_date  weekday  month  ...  is_mid_month is_last_3_days  is_day_24_non_december\n",
      "0    2016-09-04        6      9  ...             0              0                       0\n",
      "1    2016-09-05        0      9  ...             0              0                       0\n",
      "2    2016-09-06        1      9  ...             0              0                       0\n",
      "\n",
      "üìÖ DATE COLUMNS ANALYSIS:\n",
      "  purchase_date: ['2016-09-04', '2016-09-05', '2016-09-06']\n",
      "\n",
      "üîë ID COLUMNS:\n",
      "  is_holiday: 2 unique values (0.3% unique)\n",
      "  holiday_name: 9 unique values (45.0% unique)\n",
      "  is_friday: 2 unique values (0.3% unique)\n",
      "  is_shopping_holiday: 2 unique values (0.3% unique)\n",
      "  is_mid_month: 2 unique values (0.3% unique)\n",
      "\n",
      "================================================================================\n",
      "\n",
      "üìÅ FILE: olist_order_items_dataset.csv\n",
      "------------------------------------------------------------\n",
      "üìä SHAPE: 1,000 rows x 7 columns\n",
      "üíæ SIZE: 14.72 MB\n",
      "\n",
      "üìã COLUMNS:\n",
      "   1. order_id                       | object       |   0.0% null | Ex: 00010242fe8c5a6d1ba2dd792cb16214, 00018f77f2f0320c557190d7a144bdd3, 000229ec398224ef6ca0657da4fc703e\n",
      "   2. order_item_id                  | int64        |   0.0% null | Ex: 1, 1, 1\n",
      "   3. product_id                     | object       |   0.0% null | Ex: 4244733e06e7ecb4970a6e2683c13e61, e5f2d52b802189ee658865ca93d83a8f, c777355d18b72b67abbeef9df44fd0fd\n",
      "   4. seller_id                      | object       |   0.0% null | Ex: 48436dade18ac8b2bce089ec2a041202, dd7ddc04e1b6c2c614352b383efe2d36, 5b51032eddd242adc84c38acab88f23d\n",
      "   5. shipping_limit_date            | object       |   0.0% null | Ex: 2017-09-19 09:45:35, 2017-05-03 11:05:13, 2018-01-18 14:48:30\n",
      "   6. price                          | float64      |   0.0% null | Ex: 58.9, 239.9, 199.0\n",
      "   7. freight_value                  | float64      |   0.0% null | Ex: 13.29, 19.93, 17.87\n",
      "\n",
      "üìà SAMPLE DATA (first 3 rows):\n",
      "                        order_id  order_item_id                     product_id  ...  shipping_limit_date  price  freight_value\n",
      "0  00010242fe8c5a6d1ba2dd792c...              1  4244733e06e7ecb4970a6e2683...  ...  2017-09-19 09:45:35   58.9          13.29\n",
      "1  00018f77f2f0320c557190d7a1...              1  e5f2d52b802189ee658865ca93...  ...  2017-05-03 11:05:13  239.9          19.93\n",
      "2  000229ec398224ef6ca0657da4...              1  c777355d18b72b67abbeef9df4...  ...  2018-01-18 14:48:30  199.0          17.87\n",
      "\n",
      "üìÖ DATE COLUMNS ANALYSIS:\n",
      "  shipping_limit_date: ['2017-09-19 09:45:35', '2017-05-03 11:05:13', '2018-01-18 14:48:30']\n",
      "\n",
      "üîë ID COLUMNS:\n",
      "  order_id: 877 unique values (87.7% unique)\n",
      "  order_item_id: 5 unique values (0.5% unique)\n",
      "  product_id: 832 unique values (83.2% unique)\n",
      "  seller_id: 456 unique values (45.6% unique)\n",
      "\n",
      "================================================================================\n",
      "\n",
      "üìÅ FILE: olist_order_payments_dataset.csv\n",
      "------------------------------------------------------------\n",
      "üìä SHAPE: 1,000 rows x 5 columns\n",
      "üíæ SIZE: 5.51 MB\n",
      "\n",
      "üìã COLUMNS:\n",
      "   1. order_id                       | object       |   0.0% null | Ex: b81ef226f3fe1789b1e8b2acac839d17, a9810da82917af2d9aefd1278f1dcfa0, 25e8ea4e93396b6fa0d3dd708e76c1bd\n",
      "   2. payment_sequential             | int64        |   0.0% null | Ex: 1, 1, 1\n",
      "   3. payment_type                   | object       |   0.0% null | Ex: credit_card, credit_card, credit_card\n",
      "   4. payment_installments           | int64        |   0.0% null | Ex: 8, 1, 1\n",
      "   5. payment_value                  | float64      |   0.0% null | Ex: 99.33, 24.39, 65.71\n",
      "\n",
      "üìà SAMPLE DATA (first 3 rows):\n",
      "                        order_id  payment_sequential payment_type  payment_installments  payment_value\n",
      "0  b81ef226f3fe1789b1e8b2acac...                   1  credit_card                     8          99.33\n",
      "1  a9810da82917af2d9aefd1278f...                   1  credit_card                     1          24.39\n",
      "2  25e8ea4e93396b6fa0d3dd708e...                   1  credit_card                     1          65.71\n",
      "\n",
      "üîë ID COLUMNS:\n",
      "  order_id: 1,000 unique values (100.0% unique)\n",
      "\n",
      "================================================================================\n",
      "\n",
      "üìÅ FILE: olist_order_reviews_dataset.csv\n",
      "------------------------------------------------------------\n",
      "üìä SHAPE: 1,000 rows x 7 columns\n",
      "üíæ SIZE: 13.78 MB\n",
      "\n",
      "üìã COLUMNS:\n",
      "   1. review_id                      | object       |   0.0% null | Ex: 7bc2406110b926393aa56f80a40eba40, 80e641a11e56f04c1ad469d5645fdfde, 228ce5500dc1d8e020d8d1322874b6f0\n",
      "   2. order_id                       | object       |   0.0% null | Ex: 73fc7af87114b39712e6da79b0a377eb, a548910a1c6147796b98fdf73dbeba33, f9e4b658b201a9f2ecdecbb34bed034b\n",
      "   3. review_score                   | int64        |   0.0% null | Ex: 4, 5, 5\n",
      "   4. review_comment_title           | object       |  88.5% null | Ex: recomendo, Super recomendo, N√£o chegou meu produto \n",
      "   5. review_comment_message         | object       |  56.4% null | Ex: Recebi bem antes do prazo estipulado., Parab√©ns lojas lannister adorei comprar pela Inter, aparelho eficiente. no site a marca do aparelho es\n",
      "   6. review_creation_date           | object       |   0.0% null | Ex: 2018-01-18 00:00:00, 2018-03-10 00:00:00, 2018-02-17 00:00:00\n",
      "   7. review_answer_timestamp        | object       |   0.0% null | Ex: 2018-01-18 21:46:59, 2018-03-11 03:05:13, 2018-02-18 14:36:24\n",
      "\n",
      "üìà SAMPLE DATA (first 3 rows):\n",
      "                       review_id                       order_id  review_score  ... review_comment_message review_creation_date review_answer_timestamp\n",
      "0  7bc2406110b926393aa56f80a4...  73fc7af87114b39712e6da79b0...             4  ...                    NaN  2018-01-18 00:00:00     2018-01-18 21:46:59\n",
      "1  80e641a11e56f04c1ad469d564...  a548910a1c6147796b98fdf73d...             5  ...                    NaN  2018-03-10 00:00:00     2018-03-11 03:05:13\n",
      "2  228ce5500dc1d8e020d8d13228...  f9e4b658b201a9f2ecdecbb34b...             5  ...                    NaN  2018-02-17 00:00:00     2018-02-18 14:36:24\n",
      "\n",
      "üìÖ DATE COLUMNS ANALYSIS:\n",
      "  review_creation_date: ['2018-01-18 00:00:00', '2018-03-10 00:00:00', '2018-02-17 00:00:00']\n",
      "  review_answer_timestamp: ['2018-01-18 21:46:59', '2018-03-11 03:05:13', '2018-02-18 14:36:24']\n",
      "\n",
      "üîë ID COLUMNS:\n",
      "  review_id: 1,000 unique values (100.0% unique)\n",
      "  order_id: 1,000 unique values (100.0% unique)\n",
      "\n",
      "================================================================================\n",
      "\n",
      "üìÅ FILE: olist_orders_dataset.csv\n",
      "------------------------------------------------------------\n",
      "üìä SHAPE: 1,000 rows x 8 columns\n",
      "üíæ SIZE: 16.84 MB\n",
      "\n",
      "üìã COLUMNS:\n",
      "   1. order_id                       | object       |   0.0% null | Ex: e481f51cbdc54678b7cc49136f2d6af7, 53cdb2fc8bc7dce0b6741e2150273451, 47770eb9100c2d0c44946d9cf07ec65d\n",
      "   2. customer_id                    | object       |   0.0% null | Ex: 9ef432eb6251297304e76186b10a928d, b0830fb4747a6c6d20dea0b8c802d7ef, 41ce2a54c0b03bf3443c3d931a367089\n",
      "   3. order_status                   | object       |   0.0% null | Ex: delivered, delivered, delivered\n",
      "   4. order_purchase_timestamp       | object       |   0.0% null | Ex: 2017-10-02 10:56:33, 2018-07-24 20:41:37, 2018-08-08 08:38:49\n",
      "   5. order_approved_at              | object       |   0.0% null | Ex: 2017-10-02 11:07:15, 2018-07-26 03:24:27, 2018-08-08 08:55:23\n",
      "   6. order_delivered_carrier_date   | object       |   1.1% null | Ex: 2017-10-04 19:55:00, 2018-07-26 14:31:00, 2018-08-08 13:50:00\n",
      "   7. order_delivered_customer_date  | object       |   2.5% null | Ex: 2017-10-10 21:25:13, 2018-08-07 15:27:45, 2018-08-17 18:06:29\n",
      "   8. order_estimated_delivery_date  | object       |   0.0% null | Ex: 2017-10-18 00:00:00, 2018-08-13 00:00:00, 2018-09-04 00:00:00\n",
      "\n",
      "üìà SAMPLE DATA (first 3 rows):\n",
      "                        order_id                    customer_id order_status  ... order_delivered_carrier_date order_delivered_customer_date order_estimated_delivery_date\n",
      "0  e481f51cbdc54678b7cc49136f...  9ef432eb6251297304e76186b1...    delivered  ...          2017-10-04 19:55:00           2017-10-10 21:25:13           2017-10-18 00:00:00\n",
      "1  53cdb2fc8bc7dce0b6741e2150...  b0830fb4747a6c6d20dea0b8c8...    delivered  ...          2018-07-26 14:31:00           2018-08-07 15:27:45           2018-08-13 00:00:00\n",
      "2  47770eb9100c2d0c44946d9cf0...  41ce2a54c0b03bf3443c3d931a...    delivered  ...          2018-08-08 13:50:00           2018-08-17 18:06:29           2018-09-04 00:00:00\n",
      "\n",
      "üìÖ DATE COLUMNS ANALYSIS:\n",
      "  order_purchase_timestamp: ['2017-10-02 10:56:33', '2018-07-24 20:41:37', '2018-08-08 08:38:49']\n",
      "  order_delivered_carrier_date: ['2017-10-04 19:55:00', '2018-07-26 14:31:00', '2018-08-08 13:50:00']\n",
      "  order_delivered_customer_date: ['2017-10-10 21:25:13', '2018-08-07 15:27:45', '2018-08-17 18:06:29']\n",
      "  order_estimated_delivery_date: ['2017-10-18 00:00:00', '2018-08-13 00:00:00', '2018-09-04 00:00:00']\n",
      "\n",
      "üîë ID COLUMNS:\n",
      "  order_id: 1,000 unique values (100.0% unique)\n",
      "  customer_id: 1,000 unique values (100.0% unique)\n",
      "\n",
      "================================================================================\n",
      "\n",
      "üìÅ FILE: olist_orders_dataset2.csv\n",
      "------------------------------------------------------------\n",
      "üìä SHAPE: 1,000 rows x 9 columns\n",
      "üíæ SIZE: 17.64 MB\n",
      "\n",
      "üìã COLUMNS:\n",
      "   1. order_id                       | object       |   0.0% null | Ex: e481f51cbdc54678b7cc49136f2d6af7, 53cdb2fc8bc7dce0b6741e2150273451, 47770eb9100c2d0c44946d9cf07ec65d\n",
      "   2. customer_id                    | object       |   0.0% null | Ex: 9ef432eb6251297304e76186b10a928d, b0830fb4747a6c6d20dea0b8c802d7ef, 41ce2a54c0b03bf3443c3d931a367089\n",
      "   3. order_status                   | object       |   0.0% null | Ex: delivered, delivered, delivered\n",
      "   4. order_purchase_timestamp       | object       |   0.0% null | Ex: 2017-10-02 10:56:33, 2018-07-24 20:41:37, 2018-08-08 08:38:49\n",
      "   5. order_approved_at              | object       |   0.0% null | Ex: 2017-10-02 11:07:15, 2018-07-26 03:24:27, 2018-08-08 08:55:23\n",
      "   6. order_delivered_carrier_date   | object       |   1.1% null | Ex: 2017-10-04 19:55:00, 2018-07-26 14:31:00, 2018-08-08 13:50:00\n",
      "   7. order_delivered_customer_date  | object       |   2.5% null | Ex: 2017-10-10 21:25:13, 2018-08-07 15:27:45, 2018-08-17 18:06:29\n",
      "   8. order_estimated_delivery_date  | object       |   0.0% null | Ex: 2017-10-18 00:00:00, 2018-08-13 00:00:00, 2018-09-04 00:00:00\n",
      "   9. purchase_date                  | object       |   0.0% null | Ex: 2017-10-02, 2018-07-24, 2018-08-08\n",
      "\n",
      "üìà SAMPLE DATA (first 3 rows):\n",
      "                        order_id                    customer_id order_status  ... order_delivered_customer_date order_estimated_delivery_date purchase_date\n",
      "0  e481f51cbdc54678b7cc49136f...  9ef432eb6251297304e76186b1...    delivered  ...           2017-10-10 21:25:13           2017-10-18 00:00:00    2017-10-02\n",
      "1  53cdb2fc8bc7dce0b6741e2150...  b0830fb4747a6c6d20dea0b8c8...    delivered  ...           2018-08-07 15:27:45           2018-08-13 00:00:00    2018-07-24\n",
      "2  47770eb9100c2d0c44946d9cf0...  41ce2a54c0b03bf3443c3d931a...    delivered  ...           2018-08-17 18:06:29           2018-09-04 00:00:00    2018-08-08\n",
      "\n",
      "üìÖ DATE COLUMNS ANALYSIS:\n",
      "  order_purchase_timestamp: ['2017-10-02 10:56:33', '2018-07-24 20:41:37', '2018-08-08 08:38:49']\n",
      "  order_delivered_carrier_date: ['2017-10-04 19:55:00', '2018-07-26 14:31:00', '2018-08-08 13:50:00']\n",
      "  order_delivered_customer_date: ['2017-10-10 21:25:13', '2018-08-07 15:27:45', '2018-08-17 18:06:29']\n",
      "  order_estimated_delivery_date: ['2017-10-18 00:00:00', '2018-08-13 00:00:00', '2018-09-04 00:00:00']\n",
      "  purchase_date: ['2017-10-02', '2018-07-24', '2018-08-08']\n",
      "\n",
      "üîë ID COLUMNS:\n",
      "  order_id: 1,000 unique values (100.0% unique)\n",
      "  customer_id: 1,000 unique values (100.0% unique)\n",
      "\n",
      "================================================================================\n",
      "\n",
      "üìÅ FILE: olist_products_dataset.csv\n",
      "------------------------------------------------------------\n",
      "üìä SHAPE: 1,000 rows x 9 columns\n",
      "üíæ SIZE: 2.27 MB\n",
      "\n",
      "üìã COLUMNS:\n",
      "   1. product_id                     | object       |   0.0% null | Ex: 1e9e8ef04dbcff4541ed26657ea517e5, 3aa071139cb16b67ca9e5dea641aaa2f, 96bd76ec8810374ed1b65e291975717f\n",
      "   2. product_category_name          | object       |   2.9% null | Ex: perfumaria, artes, esporte_lazer\n",
      "   3. product_name_lenght            | float64      |   2.9% null | Ex: 40.0, 44.0, 46.0\n",
      "   4. product_description_lenght     | float64      |   2.9% null | Ex: 287.0, 276.0, 250.0\n",
      "   5. product_photos_qty             | float64      |   2.9% null | Ex: 1.0, 1.0, 1.0\n",
      "   6. product_weight_g               | int64        |   0.0% null | Ex: 225, 1000, 154\n",
      "   7. product_length_cm              | int64        |   0.0% null | Ex: 16, 30, 18\n",
      "   8. product_height_cm              | int64        |   0.0% null | Ex: 10, 18, 9\n",
      "   9. product_width_cm               | int64        |   0.0% null | Ex: 14, 20, 15\n",
      "\n",
      "üìà SAMPLE DATA (first 3 rows):\n",
      "                      product_id product_category_name  product_name_lenght  ...  product_length_cm  product_height_cm  product_width_cm\n",
      "0  1e9e8ef04dbcff4541ed26657e...            perfumaria                 40.0  ...                 16                 10                14\n",
      "1  3aa071139cb16b67ca9e5dea64...                 artes                 44.0  ...                 30                 18                20\n",
      "2  96bd76ec8810374ed1b65e2919...         esporte_lazer                 46.0  ...                 18                  9                15\n",
      "\n",
      "üîë ID COLUMNS:\n",
      "  product_id: 1,000 unique values (100.0% unique)\n",
      "  product_width_cm: 56 unique values (5.6% unique)\n",
      "\n",
      "================================================================================\n",
      "\n",
      "üìÅ FILE: olist_sellers_dataset.csv\n",
      "------------------------------------------------------------\n",
      "üìä SHAPE: 1,000 rows x 4 columns\n",
      "üíæ SIZE: 0.17 MB\n",
      "\n",
      "üìã COLUMNS:\n",
      "   1. seller_id                      | object       |   0.0% null | Ex: 3442f8959a84dea7ee197c632cb2df15, d1b65fc7debc3361ea86b5f14c68d2e2, ce3ad9de960102d0677a81f5d0bb7b2d\n",
      "   2. seller_zip_code_prefix         | int64        |   0.0% null | Ex: 13023, 13844, 20031\n",
      "   3. seller_city                    | object       |   0.0% null | Ex: campinas, mogi guacu, rio de janeiro\n",
      "   4. seller_state                   | object       |   0.0% null | Ex: SP, SP, RJ\n",
      "\n",
      "üìà SAMPLE DATA (first 3 rows):\n",
      "                       seller_id  seller_zip_code_prefix     seller_city seller_state\n",
      "0  3442f8959a84dea7ee197c632c...                   13023        campinas           SP\n",
      "1  d1b65fc7debc3361ea86b5f14c...                   13844      mogi guacu           SP\n",
      "2  ce3ad9de960102d0677a81f5d0...                   20031  rio de janeiro           RJ\n",
      "\n",
      "üîë ID COLUMNS:\n",
      "  seller_id: 1,000 unique values (100.0% unique)\n",
      "\n",
      "================================================================================\n",
      "\n",
      "üìÅ FILE: olitst_holiday_dataset.csv\n",
      "------------------------------------------------------------\n",
      "üìä SHAPE: 774 rows x 18 columns\n",
      "üíæ SIZE: 0.04 MB\n",
      "\n",
      "üìã COLUMNS:\n",
      "   1. purchase_date                  | object       |   0.0% null | Ex: 2016-09-04, 2016-09-05, 2016-09-06\n",
      "   2. weekday                        | int64        |   0.0% null | Ex: 6, 0, 1\n",
      "   3. month                          | int64        |   0.0% null | Ex: 9, 9, 9\n",
      "   4. is_holiday                     | int64        |   0.0% null | Ex: 0, 0, 0\n",
      "   5. holiday_name                   | object       |  97.4% null | Ex: Independ√™ncia do Brasil, Nossa Senhora Aparecida, Finados\n",
      "   6. is_carnival                    | int64        |   0.0% null | Ex: 0, 0, 0\n",
      "   7. is_weekend                     | int64        |   0.0% null | Ex: 1, 0, 0\n",
      "   8. is_friday                      | int64        |   0.0% null | Ex: 0, 0, 0\n",
      "   9. season                         | object       |   0.0% null | Ex: Spring, Spring, Spring\n",
      "  10. is_back_to_school              | int64        |   0.0% null | Ex: 0, 0, 0\n",
      "  11. christmas_season               | int64        |   0.0% null | Ex: 0, 0, 0\n",
      "  12. is_major_event                 | int64        |   0.0% null | Ex: 0, 0, 0\n",
      "  13. is_shopping_holiday            | int64        |   0.0% null | Ex: 0, 0, 0\n",
      "  14. day_of_month                   | int64        |   0.0% null | Ex: 4, 5, 6\n",
      "  15. is_first_3_days                | int64        |   0.0% null | Ex: 0, 0, 0\n",
      "  16. is_day_1                       | int64        |   0.0% null | Ex: 0, 0, 0\n",
      "  17. is_mid_month                   | int64        |   0.0% null | Ex: 0, 0, 0\n",
      "  18. is_last_3_days                 | int64        |   0.0% null | Ex: 0, 0, 0\n",
      "\n",
      "üìà SAMPLE DATA (first 3 rows):\n",
      "  purchase_date  weekday  month  ...  is_day_1 is_mid_month  is_last_3_days\n",
      "0    2016-09-04        6      9  ...         0            0               0\n",
      "1    2016-09-05        0      9  ...         0            0               0\n",
      "2    2016-09-06        1      9  ...         0            0               0\n",
      "\n",
      "üìÖ DATE COLUMNS ANALYSIS:\n",
      "  purchase_date: ['2016-09-04', '2016-09-05', '2016-09-06']\n",
      "\n",
      "üîë ID COLUMNS:\n",
      "  is_holiday: 2 unique values (0.3% unique)\n",
      "  holiday_name: 9 unique values (45.0% unique)\n",
      "  is_friday: 2 unique values (0.3% unique)\n",
      "  is_shopping_holiday: 2 unique values (0.3% unique)\n",
      "  is_mid_month: 2 unique values (0.3% unique)\n",
      "\n",
      "================================================================================\n",
      "\n",
      "üìÅ FILE: product_category_name_translation.csv\n",
      "------------------------------------------------------------\n",
      "üìä SHAPE: 71 rows x 2 columns\n",
      "üíæ SIZE: 0.00 MB\n",
      "\n",
      "üìã COLUMNS:\n",
      "   1. product_category_name          | object       |   0.0% null | Ex: beleza_saude, informatica_acessorios, automotivo\n",
      "   2. product_category_name_english  | object       |   0.0% null | Ex: health_beauty, computers_accessories, auto\n",
      "\n",
      "üìà SAMPLE DATA (first 3 rows):\n",
      "    product_category_name product_category_name_english\n",
      "0            beleza_saude                 health_beauty\n",
      "1  informatica_acessorios         computers_accessories\n",
      "2              automotivo                          auto\n",
      "\n",
      "================================================================================\n",
      "\n",
      "üìÅ FILE: state_data_sources.csv\n",
      "------------------------------------------------------------\n",
      "üìä SHAPE: 6 rows x 5 columns\n",
      "üíæ SIZE: 0.00 MB\n",
      "\n",
      "üìã COLUMNS:\n",
      "   1. category                       | object       |   0.0% null | Ex: Population Data, GDP per Capita, Internet Penetration\n",
      "   2. source                         | object       |   0.0% null | Ex: IBGE - Instituto Brasileiro de Geografia e Estat√≠s, IBGE - Contas Regionais do Brasil, ANATEL - Ag√™ncia Nacional de Telecomunica√ß√µes + TI\n",
      "   3. year                           | object       |   0.0% null | Ex: 2017 estimates, 2017, 2017\n",
      "   4. url                            | object       |   0.0% null | Ex: https://www.ibge.gov.br/estatisticas/sociais/popul, https://www.ibge.gov.br/estatisticas/economicas/co, https://www.anatel.gov.br/ and https://cetic.br/\n",
      "   5. methodology                    | object       |   0.0% null | Ex: Official population projections for 2017, closest , State GDP divided by population, in thousands of r, Percentage of households with internet access by s\n",
      "\n",
      "üìà SAMPLE DATA (first 3 rows):\n",
      "               category                         source            year                            url                    methodology\n",
      "0       Population Data  IBGE - Instituto Brasileir...  2017 estimates  https://www.ibge.gov.br/es...  Official population projec...\n",
      "1        GDP per Capita  IBGE - Contas Regionais do...            2017  https://www.ibge.gov.br/es...  State GDP divided by popul...\n",
      "2  Internet Penetration  ANATEL - Ag√™ncia Nacional ...            2017  https://www.anatel.gov.br/...  Percentage of households w...\n",
      "\n",
      "================================================================================\n",
      "\n",
      "üìÅ FILE: state_enhancement_documented.csv\n",
      "------------------------------------------------------------\n",
      "üìä SHAPE: 27 rows x 12 columns\n",
      "üíæ SIZE: 0.00 MB\n",
      "\n",
      "üìã COLUMNS:\n",
      "   1. state_code                     | object       |   0.0% null | Ex: AC, AL, AM\n",
      "   2. state_name                     | object       |   0.0% null | Ex: Acre, Alagoas, Amazonas\n",
      "   3. region                         | object       |   0.0% null | Ex: North, Northeast, North\n",
      "   4. population_2017                | int64        |   0.0% null | Ex: 829780, 3322820, 4063614\n",
      "   5. gdp_per_capita_2017            | float64      |   0.0% null | Ex: 17.284, 17.178, 22.598\n",
      "   6. internet_penetration_pct       | int64        |   0.0% null | Ex: 48, 55, 55\n",
      "   7. higher_education_pct           | float64      |   0.0% null | Ex: 7.9, 7.5, 7.8\n",
      "   8. urbanization_rate              | float64      |   0.0% null | Ex: 72.6, 73.6, 79.1\n",
      "   9. area_km2                       | float64      |   3.7% null | Ex: 164124.0, 27848.0, 1559161.0\n",
      "  10. population_density             | float64      |   0.0% null | Ex: 5.055811459628086, 119.3198793450158, 2.606282481411477\n",
      "  11. economic_tier                  | object       |   0.0% null | Ex: Lower-Middle, Lower-Middle, Middle\n",
      "  12. is_major_metro                 | int64        |   0.0% null | Ex: 0, 0, 0\n",
      "\n",
      "üìà SAMPLE DATA (first 3 rows):\n",
      "  state_code state_name     region  ...  population_density  economic_tier  is_major_metro\n",
      "0         AC       Acre      North  ...            5.055811   Lower-Middle               0\n",
      "1         AL    Alagoas  Northeast  ...          119.319879   Lower-Middle               0\n",
      "2         AM   Amazonas      North  ...            2.606282         Middle               0\n",
      "\n",
      "================================================================================\n",
      "\n",
      "üîó POTENTIAL DATA RELATIONSHIPS:\n",
      "----------------------------------------\n",
      "olist_orders_dataset2.csv: customer_id, order_id\n",
      "olist_customers_dataset.csv: customer_id\n",
      "olist_order_items_dataset.csv: order_id, product_id, seller_id\n",
      "olist_products_dataset.csv: product_id\n",
      "olist_sellers_dataset.csv: seller_id\n",
      "olist_order_payments_dataset.csv: order_id\n",
      "olist_order_reviews_dataset.csv: order_id\n",
      "\n",
      "üéØ RECOMMENDED LOADING ORDER:\n",
      "1. dim_states (from state_enhancement_documented.csv)\n",
      "2. dim_customers (from olist_customers_dataset.csv)\n",
      "3. dim_product_categories (from product_category_name_translation.csv)\n",
      "4. dim_products (from olist_products_dataset.csv)\n",
      "5. dim_sellers (from olist_sellers_dataset.csv)\n",
      "6. fact_orders (from olist_orders_dataset2.csv)\n",
      "7. dim_order_items (from olist_order_items_dataset.csv)\n",
      "8. dim_payments (from olist_order_payments_dataset.csv)\n",
      "9. dim_reviews (from olist_order_reviews_dataset.csv)\n",
      "10. dim_holidays (from olist_holiday_dataset.csv)\n",
      "11. dim_economic_indicators (from economic_indicators.csv)\n",
      "12. dim_geolocation (from olist_geolocation_dataset.csv)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "def analyze_csv_files(data_folder=\"data\"):\n",
    "    \"\"\"\n",
    "    Analyze all CSV files in the data folder and print their structure\n",
    "    \"\"\"\n",
    "    data_path = Path(data_folder)\n",
    "    \n",
    "    # Debug: Check if folder exists and list all files\n",
    "    print(f\"Looking for CSV files in: {data_path.absolute()}\")\n",
    "    print(f\"Folder exists: {data_path.exists()}\")\n",
    "    \n",
    "    if data_path.exists():\n",
    "        all_files = list(data_path.iterdir())\n",
    "        print(f\"All files in folder: {[f.name for f in all_files]}\")\n",
    "    \n",
    "    csv_files = list(data_path.glob(\"*.csv\"))\n",
    "    print(f\"Found {len(csv_files)} CSV files: {[f.name for f in csv_files]}\")\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    print(\"CSV FILES ANALYSIS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    for csv_file in sorted(csv_files):\n",
    "        print(f\"\\nüìÅ FILE: {csv_file.name}\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        try:\n",
    "            # Read CSV with minimal rows for structure analysis\n",
    "            df = pd.read_csv(csv_file, nrows=1000)  # Read first 1000 rows for analysis\n",
    "            \n",
    "            print(f\"üìä SHAPE: {df.shape[0]:,} rows x {df.shape[1]} columns\")\n",
    "            print(f\"üíæ SIZE: {os.path.getsize(csv_file) / (1024*1024):.2f} MB\")\n",
    "            \n",
    "            print(\"\\nüìã COLUMNS:\")\n",
    "            for i, col in enumerate(df.columns, 1):\n",
    "                dtype = df[col].dtype\n",
    "                null_count = df[col].isnull().sum()\n",
    "                null_pct = (null_count / len(df)) * 100\n",
    "                \n",
    "                # Get sample values (non-null)\n",
    "                sample_values = df[col].dropna().head(3).tolist()\n",
    "                sample_str = \", \".join([str(val)[:50] for val in sample_values])\n",
    "                \n",
    "                print(f\"  {i:2d}. {col:<30} | {str(dtype):<12} | {null_pct:5.1f}% null | Ex: {sample_str}\")\n",
    "            \n",
    "            print(f\"\\nüìà SAMPLE DATA (first 3 rows):\")\n",
    "            print(df.head(3).to_string(max_cols=6, max_colwidth=30))\n",
    "            \n",
    "            # Special analysis for date columns\n",
    "            date_columns = []\n",
    "            for col in df.columns:\n",
    "                if any(keyword in col.lower() for keyword in ['date', 'time', 'timestamp']):\n",
    "                    date_columns.append(col)\n",
    "            \n",
    "            if date_columns:\n",
    "                print(f\"\\nüìÖ DATE COLUMNS ANALYSIS:\")\n",
    "                for col in date_columns:\n",
    "                    sample_dates = df[col].dropna().head(3).tolist()\n",
    "                    print(f\"  {col}: {sample_dates}\")\n",
    "            \n",
    "            # Check for potential ID columns\n",
    "            id_columns = [col for col in df.columns if 'id' in col.lower()]\n",
    "            if id_columns:\n",
    "                print(f\"\\nüîë ID COLUMNS:\")\n",
    "                for col in id_columns:\n",
    "                    unique_count = df[col].nunique()\n",
    "                    total_count = len(df[col].dropna())\n",
    "                    uniqueness = (unique_count / total_count * 100) if total_count > 0 else 0\n",
    "                    print(f\"  {col}: {unique_count:,} unique values ({uniqueness:.1f}% unique)\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå ERROR reading {csv_file.name}: {str(e)}\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "def check_data_relationships():\n",
    "    \"\"\"\n",
    "    Check potential relationships between datasets\n",
    "    \"\"\"\n",
    "    print(\"\\nüîó POTENTIAL DATA RELATIONSHIPS:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    relationships = {\n",
    "        \"olist_orders_dataset2.csv\": [\"customer_id\", \"order_id\"],\n",
    "        \"olist_customers_dataset.csv\": [\"customer_id\"],\n",
    "        \"olist_order_items_dataset.csv\": [\"order_id\", \"product_id\", \"seller_id\"],\n",
    "        \"olist_products_dataset.csv\": [\"product_id\"],\n",
    "        \"olist_sellers_dataset.csv\": [\"seller_id\"],\n",
    "        \"olist_order_payments_dataset.csv\": [\"order_id\"],\n",
    "        \"olist_order_reviews_dataset.csv\": [\"order_id\"],\n",
    "    }\n",
    "    \n",
    "    for file, keys in relationships.items():\n",
    "        print(f\"{file}: {', '.join(keys)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Check if we need to go up one directory level\n",
    "    import os\n",
    "    if os.path.exists(\"data\"):\n",
    "        analyze_csv_files(\"data\")\n",
    "    elif os.path.exists(\"../data\"):\n",
    "        analyze_csv_files(\"../data\")\n",
    "    else:\n",
    "        print(\"Could not find data folder. Please run from project root directory.\")\n",
    "        print(\"Current directory:\", os.getcwd())\n",
    "    \n",
    "    check_data_relationships()\n",
    "    \n",
    "    print(\"\\nüéØ RECOMMENDED LOADING ORDER:\")\n",
    "    print(\"1. dim_states (from state_enhancement_documented.csv)\")\n",
    "    print(\"2. dim_customers (from olist_customers_dataset.csv)\")\n",
    "    print(\"3. dim_product_categories (from product_category_name_translation.csv)\")\n",
    "    print(\"4. dim_products (from olist_products_dataset.csv)\")\n",
    "    print(\"5. dim_sellers (from olist_sellers_dataset.csv)\")\n",
    "    print(\"6. fact_orders (from olist_orders_dataset2.csv)\")\n",
    "    print(\"7. dim_order_items (from olist_order_items_dataset.csv)\")\n",
    "    print(\"8. dim_payments (from olist_order_payments_dataset.csv)\")\n",
    "    print(\"9. dim_reviews (from olist_order_reviews_dataset.csv)\")\n",
    "    print(\"10. dim_holidays (from olist_holiday_dataset.csv)\")\n",
    "    print(\"11. dim_economic_indicators (from economic_indicators.csv)\")\n",
    "    print(\"12. dim_geolocation (from olist_geolocation_dataset.csv)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "693a36f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DETAILED GEOLOCATION ANALYSIS ===\n",
      "Duplicate breakdown:\n",
      "  Identical duplicates: 208,875\n",
      "  City name variations: 70,229\n",
      "  Different locations: 905\n",
      "  Total duplicates: 280,009\n",
      "\n",
      "=== EXAMPLES ===\n",
      "\n",
      "1. IDENTICAL DUPLICATES (first 3):\n",
      "   (np.int64(1001), np.float64(-23.551336655288804), np.float64(-46.63402699777831)) -> sao paulo, SP (appears 3 times)\n",
      "   (np.int64(1001), np.float64(-23.55049770690751), np.float64(-46.63433817805407)) -> sao paulo, SP (appears 8 times)\n",
      "   (np.int64(1001), np.float64(-23.5498252739339), np.float64(-46.63396956010149)) -> sao paulo, SP (appears 2 times)\n",
      "\n",
      "2. CITY NAME VARIATIONS (first 5):\n",
      "   (np.int64(1001), np.float64(-23.549779299469115), np.float64(-46.6339571183853)) -> ['s√£o paulo', 'sao paulo'] in SP (appears 2 times)\n",
      "   (np.int64(1003), np.float64(-23.5490832616594), np.float64(-46.63486400979368)) -> ['s√£o paulo', 'sao paulo'] in SP (appears 2 times)\n",
      "   (np.int64(1005), np.float64(-23.549980033585307), np.float64(-46.63476783166945)) -> ['s√£o paulo', 'sao paulo'] in SP (appears 2 times)\n",
      "   (np.int64(1005), np.float64(-23.549780031197233), np.float64(-46.63535898865553)) -> ['sao paulo', 's√£o paulo'] in SP (appears 2 times)\n",
      "   (np.int64(1007), np.float64(-23.55009663558197), np.float64(-46.637934794477474)) -> ['sao paulo', 's√£o paulo'] in SP (appears 6 times)\n",
      "\n",
      "3. DIFFERENT LOCATIONS (first 5):\n",
      "   (np.int64(1307), np.float64(-23.556811599774967), np.float64(-46.65713483250617)) -> Cities: ['sao paulo', 'sao bernardo do campo', 's√£o paulo'], States: ['SP'] (appears 6 times)\n",
      "   (np.int64(4004), np.float64(-23.57479781181837), np.float64(-46.65010622133425)) -> Cities: ['tabo√£o da serra', 's√£o paulo'], States: ['SP'] (appears 2 times)\n",
      "   (np.int64(4011), np.float64(-23.57870736918802), np.float64(-46.645778615462866)) -> Cities: ['s√£o paulo', 'sao paulo'], States: ['SP', 'AC'] (appears 14 times)\n",
      "   (np.int64(4346), np.float64(-23.64036523446758), np.float64(-46.65212213182845)) -> Cities: ['sao paulo', 'sp', 's√£o paulo'], States: ['SP'] (appears 7 times)\n",
      "   (np.int64(5026), np.float64(-23.531626162646408), np.float64(-46.69011721412089)) -> Cities: ['sp', 'sao paulo'], States: ['SP'] (appears 2 times)\n",
      "\n",
      "=== RECOMMENDATION ===\n",
      "Safe to deduplicate: 279,104 records (99.7% of duplicates)\n",
      "Need careful handling: 905 records (0.3% of duplicates)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def detailed_geolocation_analysis():\n",
    "    \"\"\"Detailed analysis of geolocation duplicates\"\"\"\n",
    "    \n",
    "    df = pd.read_csv('../data/olist_geolocation_dataset.csv')\n",
    "    \n",
    "    print(\"=== DETAILED GEOLOCATION ANALYSIS ===\")\n",
    "    \n",
    "    # Group by coordinates\n",
    "    key_columns = ['geolocation_zip_code_prefix', 'geolocation_lat', 'geolocation_lng']\n",
    "    grouped = df.groupby(key_columns)\n",
    "    \n",
    "    # Categorize duplicates\n",
    "    identical_duplicates = 0\n",
    "    city_name_variations = 0\n",
    "    different_locations = 0\n",
    "    \n",
    "    duplicate_types = {\n",
    "        'identical': [],\n",
    "        'city_variations': [],\n",
    "        'different_locations': []\n",
    "    }\n",
    "    \n",
    "    for name, group in grouped:\n",
    "        if len(group) > 1:  # Has duplicates\n",
    "            unique_cities = group['geolocation_city'].nunique()\n",
    "            unique_states = group['geolocation_state'].nunique()\n",
    "            \n",
    "            # Check if it's just accent/case differences\n",
    "            cities_lower = group['geolocation_city'].str.lower().str.normalize('NFD').str.encode('ascii', errors='ignore').str.decode('ascii')\n",
    "            normalized_cities = cities_lower.nunique()\n",
    "            \n",
    "            if unique_cities == 1 and unique_states == 1:\n",
    "                # Completely identical\n",
    "                identical_duplicates += len(group) - 1\n",
    "                duplicate_types['identical'].append({\n",
    "                    'coords': name,\n",
    "                    'count': len(group),\n",
    "                    'city': group['geolocation_city'].iloc[0],\n",
    "                    'state': group['geolocation_state'].iloc[0]\n",
    "                })\n",
    "            elif normalized_cities == 1 and unique_states == 1:\n",
    "                # Just accent/case differences\n",
    "                city_name_variations += len(group) - 1\n",
    "                duplicate_types['city_variations'].append({\n",
    "                    'coords': name,\n",
    "                    'count': len(group),\n",
    "                    'cities': group['geolocation_city'].unique(),\n",
    "                    'state': group['geolocation_state'].iloc[0]\n",
    "                })\n",
    "            else:\n",
    "                # Different locations\n",
    "                different_locations += len(group) - 1\n",
    "                duplicate_types['different_locations'].append({\n",
    "                    'coords': name,\n",
    "                    'count': len(group),\n",
    "                    'cities': group['geolocation_city'].unique(),\n",
    "                    'states': group['geolocation_state'].unique()\n",
    "                })\n",
    "    \n",
    "    print(f\"Duplicate breakdown:\")\n",
    "    print(f\"  Identical duplicates: {identical_duplicates:,}\")\n",
    "    print(f\"  City name variations: {city_name_variations:,}\")\n",
    "    print(f\"  Different locations: {different_locations:,}\")\n",
    "    print(f\"  Total duplicates: {identical_duplicates + city_name_variations + different_locations:,}\")\n",
    "    \n",
    "    # Show examples of each type\n",
    "    print(f\"\\n=== EXAMPLES ===\")\n",
    "    \n",
    "    print(f\"\\n1. IDENTICAL DUPLICATES (first 3):\")\n",
    "    for item in duplicate_types['identical'][:3]:\n",
    "        print(f\"   {item['coords']} -> {item['city']}, {item['state']} (appears {item['count']} times)\")\n",
    "    \n",
    "    print(f\"\\n2. CITY NAME VARIATIONS (first 5):\")\n",
    "    for item in duplicate_types['city_variations'][:5]:\n",
    "        print(f\"   {item['coords']} -> {list(item['cities'])} in {item['state']} (appears {item['count']} times)\")\n",
    "    \n",
    "    print(f\"\\n3. DIFFERENT LOCATIONS (first 5):\")\n",
    "    for item in duplicate_types['different_locations'][:5]:\n",
    "        print(f\"   {item['coords']} -> Cities: {list(item['cities'])}, States: {list(item['states'])} (appears {item['count']} times)\")\n",
    "    \n",
    "    # Recommendation\n",
    "    print(f\"\\n=== RECOMMENDATION ===\")\n",
    "    total_safe_to_remove = identical_duplicates + city_name_variations\n",
    "    print(f\"Safe to deduplicate: {total_safe_to_remove:,} records ({total_safe_to_remove/280009*100:.1f}% of duplicates)\")\n",
    "    print(f\"Need careful handling: {different_locations:,} records ({different_locations/280009*100:.1f}% of duplicates)\")\n",
    "    \n",
    "    return duplicate_types\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    analysis = detailed_geolocation_analysis()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
